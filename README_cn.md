PaddleSleeve
===

## 对抗样本生成与防御 AdvBox 

近年来，深度学习技术发展迅速，尤其是在计算机视觉领域内，深度学习有了越来越多的落地场景和应用场景。

对抗样本攻击（Adversarial Example Attack）可以在正常数据输入的基础上精心构造足够小的扰动，在肉眼几乎无法察觉的情况下，导致AI模型识别错误。一些如对抗贴图（Adversarial Patch）的对抗样本形式，干扰图像明显，但不影响人类的识别。目前除了图像分类，人脸识别、目标识别、 图像分割、语音识别、语义理解等各种任务都可能受到对抗样本的干扰。

Advbox收集整理了各种对抗样本的生成方法及模型加固方法，便于Paddle的开发者、使用者能更好地发现和降低模型健壮性的风险敞口。Advbox支持各种对抗样本生成的经典算法，如FGSM，BIM，JSMA和C/W等方法。

目前对抗训练是提升模型鲁棒性的主要方法，需要在训练过程中加入对抗样本到正常样本中，Advbox有相关对抗训练的示例。

## 鲁棒性评测 Robustness

业界常把ML模型抵抗输入干扰并给出正确判断的能力称为鲁棒性。一般而言，AI模型的安全分为两类：现实环境的干扰对模型可靠性（Safety）的影响，如光线明暗、“雨雪风霜”等；另一种是在人为构造的恶意攻击情况下对安全性（Security）的影响，如对抗样本。从干扰成功的难易角度上讲，定向比非定向攻击更难，黑盒比白盒攻击更难。

模型鲁棒性有三种应用场景：一般应用场景、如内容审核的强对抗场景、自动驾驶等安全攸关（safety-critical）场景。在安全攸关场景中，一些干扰的出现是极端环境的小概率事件，在数据集占比极小，甚至可能都不会存在 ，例如翻车后的车底图片、强光照耀等。由于模型未经过此类数据的训练，在强调安全的场景下其鲁棒性会受到质疑。

Robustness模块主要基于现实意义对模型鲁棒性进行评估。对图像进行的变换主要有：空间变换、色彩、明暗、天气等，模拟真实世界的失真、变形、噪点。

## 版权和许可证
PaddleSleeve由[Apache-2.0 license](LICENSE)提供